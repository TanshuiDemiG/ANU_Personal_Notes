# DisMLec19

Created: September 29, 2024 9:07 PM
Class: COMP6005
Reviewed: No

## Discrete Mathematical Models

### Lecture 19 - Kane Townsend

### Semester 2, 2024

### Next Week's Quiz Topics

- Matrices
- Counting

---

样本空间（Sample Space）是概率论和统计学中的一个重要概念。以下是对样本空间的中文解释：

### 样本空间的定义

样本空间是一个概率实验或随机过程中所有可能结果的集合。它包含了该实验或过程可能出现的所有可能性。

### 样本空间的特点

- 完备性：样本空间包含了所有可能的结果，不会遗漏任何可能性。
- 互斥性：样本空间中的每个元素（结果）是互相排斥的，即一次实验中只能出现其中一个结果。
- 确定性：在进行实验之前，样本空间就已经确定了。

### 样本空间的例子

1. 投掷一枚硬币：样本空间为 S = {正面, 反面}
2. 掷一个六面骰子：样本空间为 S = {1, 2, 3, 4, 5, 6}
3. 抽取一张扑克牌：样本空间包含52个元素，代表52张不同的扑克牌

理解样本空间对于正确计算概率和进行统计分析至关重要。它为我们提供了一个明确的框架，帮助我们分析和预测随机事件的发生。

## 1. Binomial Distribution (Revision)二项分布

### Definition

A binomial distribution has two parameters  n  and  p . It is used to calculate the probability of getting some  $k \in \mathbb{N}^*$  successes in  n  repeated independent trials, where  p  is the probability of success in a single trial.

### General Formula

$P(\text{k successes in n trials}) = \binom{n}{k} p^k (1-p)^{n-k}$ 

This formula comes from basic counting and probability principles.

### Example

A fair die is rolled 8 times:

1. What is the probability of rolling a 5 exactly three times?
$P(\text{3 successes in 8 trials}) = \binom{8}{3} \left(\frac{1}{6}\right)^3 \left(\frac{5}{6}\right)^5$ 
2. What is the probability of rolling a 3 or 5 at most one time?
 $P(\text{0 successes}) + P(\text{1 success}) = \binom{8}{0} \left(\frac{2}{6}\right)^0 \left(\frac{4}{6}\right)^8 + \binom{8}{1} \left(\frac{2}{6}\right)^1 \left(\frac{4}{6}\right)^7$ 

这个问题是关于二项分布的。让我为您解释一下如何解决这个问题：

1. 二项分布的定义：

二项分布有两个参数n和p。它用于计算在n次独立重复试验中，获得某个特定数量（记为⁍）成功的概率，其中p是单次试验成功的概率。

1. 通用公式：

题目中提到了一个通用公式，但具体内容被替换为了⁍符号。这个公式通常是基于基本的计数和概率原理。

1. 例题：

题目给出了一个例子：一个公平的骰子被投掷8次。然后提出了两个问题：

- a. 恰好投掷出三次5的概率是多少？
- b. 投掷出3或5最多一次的概率是多少？

解决这些问题需要应用二项分布的公式。虽然具体的公式和计算步骤在文本中被替换为了⁍符号，但通常的解决方法是：

1. 确定参数n（试验次数）和p（单次成功概率）。
2. 使用二项分布公式计算所需的概率。
3. 对于第二个问题，可能需要计算多个概率并相加。

要得到具体的答案，我们需要完整的公式和更多的信息。但这就是解决这类问题的基本思路。

让我为您详细解释如何计算这些二项分布问题：

### 1. 计算恰好投掷出三次5的概率

对于这个问题，我们需要使用二项分布公式：

\[ $P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$ \]

其中：

- n = 8（总共投掷8次）
- k = 3（我们想要恰好3次成功）
- p = 1/6（投掷出5的概率是1/6）

将这些值代入公式：

\[ P(X = 3) = \binom{8}{3} (1/6)^3 (5/6)^5 \]

\[ = 56 \times (1/216) \times (3125/7776) \]

\[ \approx 0.0321 \text{ 或 } 3.21\% \]

### 2. 计算投掷出3或5最多一次的概率

对于这个问题，我们需要计算投掷出0次或1次3或5的概率之和。首先，让我们定义新的参数：

- n = 8（总共投掷8次）
- p = 1/3（投掷出3或5的概率是2/6 = 1/3）

我们需要计算P(X = 0) + P(X = 1)：

\[ P(X = 0) = \binom{8}{0} (1/3)^0 (2/3)^8 = (2/3)^8 \approx 0.0390 \]

\[ P(X = 1) = \binom{8}{1} (1/3)^1 (2/3)^7 = 8 \times (1/3) \times (2/3)^7 \approx 0.1471 \]

总概率：

\[ P(X \leq 1) = P(X = 0) + P(X = 1) \approx 0.0390 + 0.1471 = 0.1861 \]

因此，投掷出3或5最多一次的概率约为0.1861或18.61%。

这就是如何具体计算这两个二项分布问题的过程。通过使用二项分布公式并正确设置参数，我们可以得到准确的概率结果。

---

## 2. Random Variables and Expected Values

### Definition

A (simple) random variable on a sample space  S  is any function  $X: S \rightarrow \mathbb{Q}$ . More generally,  $S \rightarrow \mathbb{Q}^m$  but we will stick to  m=1 .

根据您的选择，我可以为您解释随机变量的定义：

随机变量是定义在样本空间S上的函数。在简单情况下，它是一个从S到实数的函数（⁍）。更一般地，它可以是一个从S到m维实数空间的函数（⁍），但在这里我们主要讨论m=1的情况，即一维随机变量。

简单来说，随机变量是一种将随机试验的每个可能结果（样本空间中的元素）与一个数值相对应的函数。这种对应关系使我们能够用数学方法来分析和描述随机现象。

例如，在抛硬币的实验中，我们可以定义一个随机变量X，使得：

- 如果硬币正面朝上，X = 1
- 如果硬币反面朝上，X = 0

这样，我们就将抛硬币的结果（正面或反面）转化为了数值（1或0），便于进行进一步的数学分析。

随机变量被定义为一个函数是因为以下几个原因：

1. 映射关系：随机变量建立了样本空间中的事件与数值之间的映射关系。这种映射允许我们用数学方法来描述和分析随机现象。
2. 量化：通过将随机事件转化为数值，我们可以对随机现象进行量化分析。这使得我们能够应用数学工具来研究概率分布、期望值等概念。
3. 统一性：将随机变量定义为函数提供了一个统一的数学框架，可以处理各种类型的随机现象，无论是离散的还是连续的。
4. 计算便利：函数的性质使得我们可以进行各种数学运算，如求和、积分等，这对于计算概率和统计量非常重要。
5. 理论基础：将随机变量视为函数为概率论和统计学提供了坚实的数学基础，使得这些领域的理论发展更加严谨和系统化。

通过这种函数化的定义，随机变量成为了连接随机现象和数学分析之间的桥梁，极大地促进了概率论和统计学的发展。

### Notation

We denote the event ‘the random variable  X  is equal to  a ’ by  \{X=a\}  instead of the more formal  $\{s \in S | X(s) = a\}$ .

### Example

- Sample Space:  $S = \{H,T\}^3$  (set of outcomes of tossing three coins)
- Random Variable:  $X((a,b,c)) = \text{number of H’s amongst }$ a, b, c
- Event:  $\{X=2\} = \{HHT, HTH, THH \}$

### Expected Value

Relative to a probability density function  P: S \rightarrow \mathbb{Q}^+ , the expected value  E(X)  of a random variable  X  is defined by:
\[ $E(X) = \sum_{s \in S} P(s) X(s) = \sum_{a \in \text{Range}(X)} P(\{X=a\}) a$ \]

### Example (cont.)

\[ $E(X) = \left(\frac{1}{8}\right)0 + \left(\frac{3}{8}\right)1 + \left(\frac{3}{8}\right)2 + \left(\frac{1}{8}\right)3 = \frac{12}{8} = 1.5$ \]

Thus, the expected value of  X  is just the average number of heads obtained when three coins are tossed.

---

## 3. Die Roll Example of Expected Value

### Game

Costs $2 to play. Roll a die. Win $10 if you get a 6. Should you expect to make or lose money? How much?

### Setup

- Sample Space:  S = $\{1, 2, 3, 4, 5, 6\}$
- Probability:  $P(j) = \frac{1}{6}$  for all  $j \in \{1, ..., 6\}$
- Gain (Loss) Function:
\[
$X(j) =
\begin{cases}
8 & \text{if } j=6 \\
-2 & \text{otherwise}
\end{cases}$
\]

### Expected Value Calculation

\[ $E(X) = \sum_{j=1}^{6} \frac{1}{6} \times X(j) = 5 \left(\frac{1}{6} \times -2\right) + \left(\frac{1}{6} \times 8\right) = -\frac{2}{6} = -\frac{1}{3}$

On average, you expect to lose $1/3 per game. In other words, if you play this game 30 times, you should expect to lose $30 \times \frac{1}{3} = 10 dollars.$

---

## 4. Independent Events

### Definition

For a sample space  S  with probability density function  P: S $\rightarrow \mathbb{Q}^+ ,  E, F \in P(S)$  are called independent events when:
\[ $P(E \cap F) = P(E) \times P(F)$ \]

根据您的选择，我可以为您解释独立事件的定义：

对于一个具有概率密度函数 P: S⁍ 的样本空间 S，当满足以下条件时，事件 A 和 B 被称为独立事件：

\[ ⁍ \]

这个定义的含义是：

- 两个事件 A 和 B 的联合概率（即它们同时发生的概率）等于它们各自发生概率的乘积。

换句话说，一个事件的发生不会影响另一个事件发生的概率。这是独立性的核心概念。

在实际应用中，独立事件的概念非常重要，因为它允许我们简化许多概率计算，特别是在处理复杂的随机过程时。

### Illustration

Toss two coins:

- Sample Space:  $S = \{H, T\}^2 = \{HH, HT, TH, TT\}$  with equally likely outcomes.
- Events:
    - $E = \{HH, HT\}$  (1st coin gives Head),  $P(E) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$
    - $F = \{HT, TT\}$  (2nd coin gives Tail),  $P(F) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$

### Independence Check

 $P(E \cap F) = P(\{HT\}) = \frac{1}{4} = \frac{1}{2} \times \frac{1}{2} = P(E) \times P(F)$ 

### Non-Independence Example

- Events:
    - $G = \{HT, TH, HH\}$  (at least one Head),  $P(G) = \frac{1}{4} + \frac{1}{4} + \frac{1}{4} = \frac{3}{4}$
    - $K = \{TH, HT, TT\}$  (at least one Tail),  $P(K) = \frac{1}{4} + \frac{1}{4} + \frac{1}{4} = \frac{3}{4}$

 $P(G \cap K) = P(\{HT, TH\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2} \neq \frac{9}{16} = \frac{3}{4} \times \frac{3}{4} = P(G) \times P(K)$ 

---

## 5. Independent Random Variables

### Definition

For a sample space  S  with probability density function  $P: S \rightarrow \mathbb{Q}^+ ,  X, Y: S \rightarrow \mathbb{Q}$  are called independent random variables when:
 $\forall a \in \text{Range}(X) \forall b \in \text{Range}(Y), \{X=a\}, \{Y=b\} \text{ are independent}$ 

### Illustration

- Toss two coins: getting  T  on the second toss is independent of getting  H  on the first.
- Random Variables:  X, Y  number of heads (0 or 1) on the 1st, 2nd toss respectively.

 $P(\{X=a\}) = P(\{Y=b\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ 
$P(\{X=a\} \cap \{Y=b\}) = \frac{1}{4}$ 

Thus,  X  and  Y  are independent.

---

## 6. Independent Random Variables — Example

### Example

Toss a regular fair die:

- Sample Space:  $S = \{1, \ldots, 6\} ,  P(i) = \frac{1}{6}, i = 1, \ldots, 6$
- Random Variables:
\[
$\begin{array}{c|cccccc}
s & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
\text{s mod 2} = X(s) & 1 & 0 & 1 & 0 & 1 & 0 \\
\text{s mod 3} = Y(s) & 1 & 2 & 0 & 1 & 2 & 0 \\
\end{array}$
\]

### Probabilities

\[
$\begin{array}{c|ccc}
a & 0 & 1 & 2 \\
\hline
P(\{X=a\}) & \frac{1}{2} & \frac{1}{2} & 0 \\
P(\{Y=a\}) & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
\end{array}$
\]

### Independence Check

For any  $a \in \{0,1\}, b \in \{0,1,2\}$ :
\[ P(\{X=a\} \cap \{Y=b\}) = \frac{1}{6} = \frac{1}{2} \times \frac{1}{3} = P(\{X=a\}) \times P(\{Y=b\}) \]

---

## 7. Non-Independent Random Variables — Example

### Example

Toss a regular fair die:

- Sample Space:  S = \{1, \ldots, 6\} ,  P(i) = \frac{1}{6}, i = 1, \ldots, 6
- Random Variables:
\[
\begin{array}{c|cccccc}
s & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
\text{s mod 3} = Y(s) & 1 & 2 & 0 & 1 & 2 & 0 \\
\text{s mod 4} = Z(s) & 1 & 2 & 3 & 0 & 1 & 2 \\
\end{array}
\]

### Probabilities

\[
\begin{array}{c|cccc}
a & 0 & 1 & 2 & 3 \\
\hline
P(\{Y=a\}) & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 \\
P(\{Z=a\}) & \frac{1}{6} & \frac{1}{3} & \frac{1}{3} & \frac{1}{6} \\
\end{array}
\]

### Non-Independence Check

\[ P(\{Y=0\} \cap \{Z=0\}) = 0 \neq \frac{1}{3} \times \frac{1}{6} = P(\{Y=0\}) \times P(\{Z=0\}) \]

Thus, the random variables  Y  and  Z  are not independent.

**Challenge:** Are the random variables  X  and  Z  independent?

---

## 8. Conditional Probability and Bayes’ Theorem

### Definition

Consider a probability experiment with sample space  S . If  $A, B \subseteq S$  and  $P(A) \neq 0$ , then the conditional probability of  B  given  A , denoted  $P(B|A)$ , is:
 $P(B|A) = \frac{P(A \cap B)}{P(A)}$ 

### Example 1

**Problem:** I toss two fair coins but only I can see the outcome. You ask, “Did they both come up tails?" I say, “No." What is the probability that both coins came up heads?

**Solution:**

- Sample Space:  $S = \{HH, HT, TH, TT\}$
- Event  A : Did not both come up tails  $A = \{HH, HT, TH\}$
- Event  B : Both coins came up heads  $B = \{HH\}$

\[ $P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(\{HH\})}{P(\{HH, HT, TH\})} = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}$ \]

### Example 2

**Problem:** A pair of fair 6-sided dice, one red and one blue, are rolled. What is the probability that the sum of the numbers showing face up is 8, given that both of the numbers are even?

**Solution:**

- Sample Space:  S = \{1, 2, 3, 4, 5, 6\} \times \{1, 2, 3, 4, 5, 6\} ,  |S| = 36
- Event  B : Sum of numbers is 8  B = \{(2,6), (3,5), (4,4), (5,3), (6,2)\}
- Event  A : Both numbers are even  A = \{(2,2), (2,4), (2,6), (4,2), (4,4), (4,6), (6,2), (6,4), (6,6)\}

\[ P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(\{(2,6), (4,4), (6,2)\})}{P(\{(2,2), (2,4), (2,6), (4,2), (4,4), (4,6), (6,2), (6,4), (6,6)\})} = \frac{\frac{3}{36}}{\frac{9}{36}} = \frac{1}{3} \]

---

## 9. Bayes’ Theorem

### Theorem

For any probability experiment with sample space  S , for any  $n \in \mathbb{N}$ , for any partition $\{B_1, B_2, \ldots, B_n\}$ of  S  and for any event  $A \subseteq S$ , if  $P(A) \neq 0$  and for all  $i \in \{1, 2, \ldots, n\}$  we have  $P(B_i) \neq 0$ , then for all  $k \in \{1, 2, \ldots, n\}$  we have:

[ $P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i=1}^{n} P(A|B_i)P(B_i)}$ 

贝叶斯定理是一个重要的概率论公式，用于计算条件概率。根据给定的信息，贝叶斯定理可以表述如下：

对于任何概率实验，给定样本空间 S，对于任何事件 A 和 S 的一个划分 {B₁, B₂, ..., Bₙ}，如果 P(A) ≠ 0 且对所有 i ∈ {1, 2, ..., n} 都有 P(Bᵢ) ≠ 0，那么对于所有 k ∈ {1, 2, ..., n}，我们有：

$P(Bₖ|A) = [P(A|Bₖ)P(Bₖ)] / [∑ᵢP(A|Bᵢ)P(Bᵢ)]$

这个公式的含义是：

- P(Bₖ|A) 是在事件 A 发生的条件下，事件 Bₖ 发生的概率。
- P(A|Bₖ) 是在事件 Bₖ 发生的条件下，事件 A 发生的概率。
- P(Bₖ) 是事件 Bₖ 的先验概率。
- 分母 ∑ᵢP(A|Bᵢ)P(Bᵢ) 是事件 A 的总概率。

贝叶斯定理的重要性在于它允许我们根据新的证据（事件 A）来更新我们对某个假设（事件 Bₖ）的信念。这在很多领域都有广泛应用，例如医学诊断、机器学习和数据分析等。

### Proof

Consider a probability experiment with sample space  S . Let  $n \in \mathbb{N} , let \{B_1, B_2, \ldots, B_n\}$ be a partition of  S  and let  A \subseteq S . Suppose that  P(A) \neq 0  and for all  $i \in \{1, 2, \ldots, n\}$  we have  $P(B_i) \neq 0 . Let  k \in \{1, 2, \ldots, n\}$ . Now:
\[ $P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A \cap S)} = \frac{P(A|B_k)P(B_k)}{P(A \cap (B_1 \cup B_2 \cup \ldots \cup B_n))}$ \]

Since $\{B_1, \ldots, B_n\}$ is a partition of  S :
\[ $P(B_k|A) = \frac{P(A|B_k)P(B_k)}{P((A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_n))} = \frac{P(A|B_k)P(B_k)}{P(A \cap B_1) + P(A \cap B_2) + \ldots + P(A \cap B_n)}$ \]

Applying the sum rule:
$P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i=1}^{n} P(A|B_i)P(B_i)}$ 

---

## 10. Applications of Bayes’ Theorem

- Solving Monty Hall problem
- Drug testing
- Disease testing
- Defective item rates

### Example 9.9.3 from Epp

Consider a medical test that screens for a disease found in 5 people in 1,000. Suppose that the false positive rate is 3% and the false negative rate is 1%. Then 99% of the time a person who has the condition tests positive for it, and 97% of the time a person who does not have the condition tests negative for it.

**Problem:**
a. What is the probability that a randomly chosen person who tests positive for the disease actually has the disease?
b. What is the probability that a randomly chosen person who tests negative for the disease does not in fact have the disease?

**Solution:**
Consider a random person from those screened. Let  A  be the event they test positive,  B_1  the event they have the disease,  B_2  the event they do not have the disease. Then:
 $P(A|B_1) = 0.99, P(A^c|B_1) = 0.01, P(A^c|B_2) = 0.97, P(A|B_2) = 0.03$